# BUILDING STAGE 3

## PROJECT CONTEXT
You are building Stage 3 of the scheduling engine - a mission-critical data compilation system that transforms validated CSV inputs into solver-agnostic, mathematically guaranteed data structures. This stage serves as the universal foundation for all subsequent pipeline stages (4-7) and must exhibit complete reliability with zero tolerance for error.

## MATHEMATICAL FOUNDATIONS & THEORETICAL COMPLIANCE
Your implementation must rigorously adhere to All of the Theoretical Foundations & Mathematical Framework, implementing these core theorems:

### CORE THEOREMS TO IMPLEMENT:
1. **Information Preservation Theorem (5.1)**: I_compiled ≥ I_source - R + I_relationships
2. **Query Completeness Theorem (5.2)**: All CSV queries remain answerable in O(log N) or better
3. **Normalization Theorem (3.3)**: Lossless BCNF with dependency preservation
4. **Relationship Discovery Theorem (3.6)**: P(R_found ⊇ R_true) ≥ 0.994
5. **Index Access Theorem (3.9)**: Point queries O(1) expected, range queries O(log N + k)

## ARCHITECTURAL SPECIFICATIONS

### FOUR-LAYER COMPILATION ARCHITECTURE:

#### LAYER 1: RAW DATA NORMALIZATION
**Package**: `stage_3/data_normalizer/`
**Responsibility**: Transform validated CSVs into lossless BCNF-compliant normalized entities

**Required Modules:**
1. `csv_ingestor.py` - CSV discovery and loading with checksum validation
2. `schema_validator.py` - Pydantic model enforcement against HEI data model
3. `dependency_validator.py` - Functional dependency verification and BCNF decomposition
4. `redundancy_eliminator.py` - Duplicate detection and removal with multiplicity preservation
5. `checkpoint_manager.py` - Layer transition validation and rollback capability
6. `normalization_engine.py` - Orchestrator implementing Theorem 3.3 guarantees

**CRITICAL IMPLEMENTATION REQUIREMENTS:**
- Implement bijective mapping preservation for Information Preservation Theorem
- Enforce all functional dependencies from HEI data model schema
- Apply lossless join property verification
- Record checksums and row counts for integrity validation
- Complexity: O(N log N) normalization time

#### LAYER 2: RELATIONSHIP DISCOVERY & MATERIALIZATION
**Module**: `relationship_engine.py`
**Responsibility**: Discover and materialize entity relationships with mathematical completeness

**Required Implementation:**
- **Syntactic Detection**: Primary-foreign key inference (100% precision)
- **Semantic Detection**: Attribute name/domain similarity analysis
- **Statistical Detection**: Value distribution correlation analysis
- **Transitive Closure**: Floyd-Warshall algorithm for complete relationship graph
- **NetworkX Integration**: Store relationships as directed graphs
- **Completeness Validation**: Achieve P(R_found ⊇ R_true) ≥ 0.994 per Theorem 3.6

#### LAYER 3: MULTI-MODAL INDEX CONSTRUCTION
**Module**: `index_builder.py`
**Responsibility**: Build optimized indices for fast query access

**Required Index Types:**
- **Hash Indices**: O(1) primary key lookups using Python dictionaries
- **B-Tree Indices**: O(log N) range queries (implement or use library)
- **Graph Indices**: O(d) relationship traversal via adjacency lists
- **Bitmap Indices**: Categorical attribute filtering
- **Performance Validation**: Verify Theorem 3.9 complexity guarantees

#### LAYER 4: UNIVERSAL DATA STRUCTURING
**Modules**: `optimization_views.py`, `compilation_engine.py`, `validation_engine.py`
**Responsibility**: Assemble solver-agnostic universal data structures

**CRITICAL CHANGE**: NO SOLVER-SPECIFIC VIEWS - create only general data structures
All solver families (PuLP, OR-Tools, DEAP, PyGMO) will handle their own input remodeling in Stage 6

## PERSISTENCE & MEMORY MANAGEMENT

### SERIALIZATION REQUIREMENTS:
**Output Files Structure:**
```
{execution_dir}/
├── Lraw.parquet          # Normalized entity tables (Apache Parquet)
├── Lrel.graphml          # Relationship graphs (NetworkX GraphML)
├── Lidx_hash.parquet     # Hash indices
├── Lidx_btree.feather    # B-tree indices  
├── Lidx_bitmap.binary    # Bitmap indices
├── manifest.json         # Schema versions, checksums, metadata
└── checkpoints/          # Layer-wise snapshots for rollback
    ├── layer1_checkpoint.pkl
    ├── layer2_checkpoint.pkl
    └── layer3_checkpoint.pkl
```

### MEMORY CONSTRAINTS:
- **Peak RAM**: ≤ 512 MB (empirically validated at 400-450 MB worst case)
- **Storage Efficiency**: O(N log N) space complexity
- **I/O Optimization**: Use memory-mapped files for large indices
- **Chunked Processing**: Stream large datasets to maintain memory limits

## INPUT/OUTPUT SPECIFICATIONS

### INPUTS:
- Stage 1 validated, stage 2 batch outputs (student_batches.csv, batch_student_membership.csv, batch_course_enrollment.csv), dynamic paramters (EAV model), hei_timetabling_datamodel.sql

### OUTPUTS:
- Normalized entity tables with referential integrity guaranteed
- Materialized relationship graphs with transitive closures
- Multi-modal indices optimized for O(log N) queries
- Universal data structures consumable by all solver families
- Metadata and checkpoint files for system integrity

-------------------------------------------------
NOTE THAT YOU'RE TO THOROUGHLY UNDERSTAND AND CAREFULLY CONSIDER DYNAMIC PARAMETER MODEL (REFER FRAMEWORKS & FOUNDATIONS AS WELL) TO CLEARLY EVALUATE THEM AND INTEGRATE THEM INTO OVERALL COMPILED DATA MODEL -- BE EXTREMELY RIGOROUS AND STRONLGY PERFECT & DISCIPLINED IN THIS PARTICULAR AREA (this is very crucial as it is an important feature and needs to be handled very carefully or we might suffer with invalid timetables later, You and I both will suffer in development of further stages, we will suffer heavy issues that will be unresolvable)

Dynamic parameters—defined via the EAV (Entity–Attribute–Value) model—are a first-class feature of our scheduling system. They were validated in Stage 1 and exercised in Stage 2, and must be seamlessly integrated into the canonical Stage 3 data compiler without adding complexity or jeopardizing rigor. Here is the end-to-end plan for evaluating and weaving dynamic parameters through each of Stage 3’s four layers, fully aligned with our mathematical foundations, single-threaded design, oblivious caching, and modular architecture:

1. Layer 1 – Raw Data Normalization  
   -  Input: `dynamic_parameters.csv` alongside all other Stage 1/2 CSVs.  
   -  Process:  
     – **Schema Validation**: Enforce that EAV triples conform to `hei_timetabling_datamodel.sql` definitions for the `dynamic_parameters` table (column names: `entity_type`, `entity_id`, `parameter_code`, `value`, etc.).  
     – **Normalization**: Treat each EAV row as an entity instance in BCNF; no decomposition needed because the EAV model is already third-normal form by design.  
     – **Redundancy Elimination**: Identify duplicate parameter settings (identical EAV triples) and remove them with multiplicity tracking.  
   -  Risk Mitigation: Ensures no malformed or duplicate parameters propagate downstream.  

2. Layer 2 – Relationship Discovery & Materialization  
   -  Input: normalized `dynamic_parameters` table.  
   -  Process:  
     – **PK–FK Inference**: Recognize that `entity_id` in dynamic parameters references primary keys in other entities (students, courses, batches, etc.). Materialize these directed edges in the relationship graph.  
     – **Semantic Links**: Infer higher-level links (e.g., a parameter code “SEGREGATE_ACADEMIC_YEAR” implies an academic_year attribute in the student entity). Add weighted semantic edges.  
     – **Transitive Closure**: Compute how dynamic parameters propagate relationships—e.g., a department-level parameter applies to all courses in that department.  
   -  Risk Mitigation: Theorem 3.6 guarantees ≥ 99.4% completeness, preventing lost parameter-entity associations.  

3. Layer 3 – Multi-Modal Index Construction  
   -  Input: materialized relationships including dynamic-parameter edges.  
   -  Process:  
     – **Hash Indices** on (`entity_type`, `parameter_code`) for O(1) lookups of parameter lists for any entity.  
     – **B-Tree Indices** on `value` for range queries on parameters (e.g., numeric thresholds).  
     – **Graph Indices** embedding dynamic-parameter edges to enable fast traversal from any entity to its relevant parameters.  
     – **Bitmap Indices** for categorical parameter codes to accelerate filtering.  
   -  Risk Mitigation: Theorem 3.9 ensures index completeness and access bounds, so no parameter lookup becomes a bottleneck.  

4. Layer 4 – Universal Data Structuring  
   -  Input: normalized entities, relationships, indices—all including dynamic parameters.  
   -  Process:  
     – **Aggregate** dynamic parameters into each entity’s in-memory dataclass as a typed dictionary or Pydantic model field (`parameters: Dict[str, Any]`).  
     – **Expose** a unified API: `CompiledData.get_parameters(entity_type, entity_id) → Dict[str, Any]`.  
     – **Persist** these associations in the final serialized structures (Parquet for tables, GraphML for graphs, index files), with dynamic parameters in `Lraw.parquet` alongside other entity attributes.  
   -  Risk Mitigation: Theorems 5.1 and 5.2 guarantee that including dynamic parameters preserves all semantic information and query completeness.

**Foundations Compliance & Rigor**  
- Every step leverages the Stage 3 Data Preservation Theorem (5.1), Query Completeness Theorem (5.2), Relationship Discovery Theorem (3.6), and Index Construction Theorem (3.9).  
- Single-threaded, sequential processing avoids concurrency risks while respecting O(N log² N) time and O(N log N) space bounds.  
- Oblivious caching and memory-mapped persistence ensure RAM never exceeds 512 MB, even when materializing dynamic-parameter indices.

**Conclusion**  
Dynamic parameters are ingested, normalized, related, indexed, and structured with the **same mathematical rigor** as all other entities. Risks of lost or misassociated parameters are mitigated by theorem-backed completeness and preservation proofs. This end-to-end integration fully complies with the formal frameworks and guarantees a reliable, error-free foundation for all downstream scheduling stages.

--------------------------------------------------------------

## ERROR MITIGATION STRATEGY

### MATHEMATICAL VALIDATION (NOT DATA RE-VALIDATION):
- **Information Preservation Validator**: Verify bijective mappings using Shannon entropy
- **Query Completeness Validator**: Test representative query set against compiled structures
- **Dependency Validator**: Ensure BCNF compliance and lossless joins
- **Relationship Validator**: Verify completeness probability ≥ 0.994

### SYSTEM-LEVEL ERROR RECOVERY:
- **Checkpoint-based rollback** between compilation layers
- **Light transitional checks**: Record counts, key presence, checksum validation
- **No redundant data validation**: Rely on Stage 1/2 mathematical proofs
- **Memory corruption detection**: Bounds checking and integrity verification

## PERFORMANCE REQUIREMENTS

### COMPLEXITY GUARANTEES:
- **Compilation Time**: O(N log² N)
- **Space Usage**: O(N log N)
- **Query Time**: O(log N) average access
- **Cache Performance**: ≤ 1/√B miss rate where B = block size

### SCALABILITY:
- Handle 1-2k students, courses, faculty, rooms efficiently
- Runtime: < 5-10 minutes total compilation
- single-threaded implementation

## TECHNOLOGY STACK

### REQUIRED LIBRARIES:
- **pandas ≥ 2.0.3**: CSV processing and DataFrame operations
- **numpy ≥ 1.24.4**: Numerical computations and array operations
- **networkx ≥ 3.2.1**: Graph structures for relationships
- **pydantic ≥ 2.5.0**: Schema validation and data models
- **scipy ≥ 1.11.4**: Statistical analysis and optimization
- **structlog ≥ 23.2.0**: Structured logging
- **typing, abc, dataclasses**: Type hints and abstract base classes

### SERIALIZATION:
- **Apache Parquet**: Columnar storage for entity tables
- **Feather**: Fast serialization for indices
- **GraphML**: Standard format for relationship graphs
- **Binary formats**: Custom serialization for bitmap indices

## IMPLEMENTATION DECISIONS - FINAL

### VALIDATED DESIGN CHOICES:
1. **Static Compile-and-Forward**: Single execution, deterministic output
2. **No Redundant Validation**: Trust Stage 1/2 & mathematical proofs
3. **Full Mathematical Guarantees**: Implement all theoretical theorems
4. **Pre-indexing Strategy**: Build all indices during compilation
5. **Single-threaded Execution**: Avoid concurrency complexity
6. **General Data Structures**: No solver-specific transformations
7. **Checkpoint Recovery**: System-level fault tolerance only

## CODE QUALITY STANDARDS

### MATHEMATICAL RIGOR:
- Embed formal theorem assertions in code as runtime checks
- Include mathematical proof references in docstrings
- Implement complexity validators to verify O(log N) performance
- Unit tests must validate theoretical guarantees

### PRODUCTION STANDARDS:
- complete error handling with structured logging
- Type hints for all functions and classes using typing module
- Abstract base classes for extensibility
- Dataclasses for clean data structures
- Professional documentation suitable for SIH judges

### DEBUGGING CAPABILITY:
- Detailed audit trails for all compilation steps
- Performance metrics collection at each layer
- Memory usage monitoring and reporting
- Clear error messages with mathematical context

## CRITICAL SUCCESS METRICS

1. **Mathematical Correctness**: All theorems implemented and validated
2. **Information Preservation**: Zero semantic data loss during compilation  
3. **Query Performance**: 100-1000x speedup over direct CSV processing
4. **Memory Efficiency**: < 512 MB peak usage with O(N log N) scaling
5. **Completeness**: All source queries answerable on compiled structures
6. **Reliability**: Zero-error production usage capability

## FINAL IMPLEMENTATION MANDATE

Build a mathematically rigorous, production-ready Stage 3 Data Compilation system that:
- Transforms heterogeneous CSV data into optimized, universal structures
- Preserves all semantic information with mathematical guarantees
- Provides logarithmic query performance for all downstream stages
- Serves as the single source of truth for Stages 4-7
- Operates within strict memory and runtime constraints
- Exhibits zero tolerance for data corruption or semantic loss

Your implementation must be suitable for integrating with stage 2 & 4 along with immediate usage in the demonstration environment with complete reliability and mathematical correctness.

## DECISION TREE SUMMARY

### DATA VALIDATION APPROACH:
❌ **REJECTED**: Redundant validation (paranoia, computational waste)  
✅ **ADOPTED**: Mathematical theorem enforcement with light transitional checks

### PROCESSING MODEL:
❌ **REJECTED**: Multi-threading (complexity, power consumption, race conditions)  
❌ **REJECTED**: On-demand processing (conditional complexity, unpredictable memory)  
✅ **ADOPTED**: Single-threaded, static compile-and-forward model

### INDEX STRATEGY:
❌ **REJECTED**: Lazy indexing (complex invalidation logic)  
✅ **ADOPTED**: Aggressive pre-indexing (matches usage pattern, predictable performance)

### SOLVER INTEGRATION:
❌ **REJECTED**: Solver-specific optimization views in Stage 3  
✅ **ADOPTED**: Universal data structures, solver will have its own remodeling in Stage 6

### ERROR RECOVERY:
❌ **REJECTED**: Complex transaction systems (unnecessary complexity)  
✅ **ADOPTED**: Checkpoint-based rollback for system-level faults only

### MEMORY MANAGEMENT:
❌ **REJECTED**: Hierarchical caching simulation (high implementation complexity)  
✅ **ADOPTED**: OS-level caching with memory-mapped files for large indices

---------------------------------

###**DIRECTORY STRUCTURE FOR STAGE 3**
stage_3/
├── __init__.py                         # Main package initializer, orchestrator interface
├── data_normalizer/                    # Layer 1: Raw Data Normalization
│   ├── __init__.py
│   ├── csv_ingestor.py                 # CSV file discovery, reading, integrity checks
│   ├── schema_validator.py             # Schema conformance checks via Pydantic models
│   ├── dependency_validator.py         # Functional dependencies & BCNF normalization verifier
│   ├── redundancy_eliminator.py        # Duplicate detection and elimination
│   ├── checkpoint_manager.py           # Checkpoint creation and lazy validation
│   └── normalization_engine.py         # Layer 1 orchestrator integrating all above modules
├── relationship_engine.py              # Layer 2: Relationship discovery & materialization
├── index_builder.py                    # Layer 3: Multi-modal index construction (hash, B-tree, graph, bitmap)
├── optimization_views.py               # Layer 4: Universal solver-agnostic data structuring
├── memory_optimizer.py                 # Storage layout optimization for cache-oblivious design
├── compilation_engine.py               # Master orchestrator integrating all layers sequentially
├── validation_engine.py                # Lightweight validation engine for transitional checks
├── performance_monitor.py              # Profiling, bottlenecks, resource usage metrics
├── storage_manager.py                  # Serialization/deserialization & checkpoint persistence
└── api_interface.py                    # REST API for status queries for pushing & pulling data from db volume given path
---------------------------------------------------
