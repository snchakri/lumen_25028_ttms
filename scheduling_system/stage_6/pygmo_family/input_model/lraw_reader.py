"""
L_raw Reader for Stage 3 Compiled Data

Reads normalized entity data from Parquet files generated by Stage 3.

Theoretical Foundation:
- Stage-3 DATA COMPILATION - Theoretical Foundations & Mathematical Framework
- Section: L_raw Layer - Normalized Entity Storage
"""

import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
import uuid


@dataclass
class EntityData:
    """Container for entity data with metadata"""
    entity_name: str
    data: pd.DataFrame
    row_count: int
    column_count: int
    primary_key: str
    foreign_keys: List[str] = field(default_factory=list)
    
    def validate(self) -> bool:
        """Validate entity data integrity"""
        # Check primary key exists
        if self.primary_key not in self.data.columns:
            return False
        
        # Check no null primary keys
        if self.data[self.primary_key].isnull().any():
            return False
        
        # Check primary key uniqueness
        if self.data[self.primary_key].duplicated().any():
            return False
        
        return True


class LRawReader:
    """
    Reader for L_raw Parquet files from Stage 3 output.
    
    L_raw contains all normalized entities as columnar Parquet files:
    - institutions.parquet
    - departments.parquet
    - programs.parquet
    - courses.parquet
    - shifts.parquet
    - time_slots.parquet
    - faculty.parquet
    - rooms.parquet
    - equipment.parquet
    - students.parquet
    - faculty_course_competency.parquet
    - constraints.parquet
    - dynamic_parameters.parquet (if exists)
    - And any other normalized entities
    """
    
    # Expected entity names and their primary keys
    ENTITY_SCHEMA = {
        'institutions': 'institution_id',
        'departments': 'department_id',
        'programs': 'program_id',
        'courses': 'course_id',
        'shifts': 'shift_id',
        'time_slots': 'timeslot_id',
        # Common alias variants are handled below (e.g., 'timeslots' -> 'time_slots')
        'faculty': 'faculty_id',
        'rooms': 'room_id',
        'equipment': 'equipment_id',
        'students': 'student_id',
        'batches': 'batch_id',
        # faculty_course_competency often lacks a single PK; we will synthesize one if missing
        'faculty_course_competency': 'competency_id',
        'constraints': 'constraint_id',
        # dynamic entities have variant PK names across builds
        'dynamic_constraints': 'constraint_id',
        'dynamic_parameters': 'param_id',
        'course_prerequisites': 'prerequisite_id',
        'room_department_access': 'access_id',
        'batch_course_enrollment': 'enrollment_id',
        'scheduling_sessions': 'session_id',
    }

    # Map non-canonical file/entity names to canonical ones
    ENTITY_NAME_ALIASES = {
        'timeslots': 'time_slots',
        'student_batches': 'batches',
    }

    # Known alternate PK column names per entity
    PRIMARY_KEY_ALIASES = {
        'time_slots': ['timeslot_id', 'slot_id', 'id'],
        'dynamic_parameters': ['parameter_id', 'param_id', 'id'],
        'dynamic_constraints': ['constraint_id', 'id'],
        'faculty_course_competency': ['competency_id', 'id'],
        'batches': ['batch_id', 'id'],
        'students': ['student_id', 'id'],
        'faculty': ['faculty_id', 'id'],
        'rooms': ['room_id', 'id'],
    }
    
    def __init__(self, input_dir: Path, logger: Optional[Any] = None):
        """
        Initialize L_raw reader.
        
        Args:
            input_dir: Path to Stage 3 output directory
            logger: Optional StructuredLogger instance
        """
        self.input_dir = Path(input_dir)
        self.logger = logger
        
        # Resolve possible Stage-3 layouts: prefer 'L_raw', fallback to 'entities'
        candidate_dirs = [self.input_dir / 'L_raw', self.input_dir / 'entities']
        self.lraw_dir = next((d for d in candidate_dirs if d.exists()), candidate_dirs[0])
        
        # Loaded entities
        self.entities: Dict[str, EntityData] = {}
        
        # Validation
        if not self.lraw_dir.exists():
            raise FileNotFoundError(f"Stage-3 entity directory not found. Tried: {candidate_dirs}")
        
        if self.logger:
            self.logger.info(f"Entity reader initialized: {self.lraw_dir}")
    
    def load_all_entities(self) -> Dict[str, EntityData]:
        """
        Load all Parquet files from L_raw directory.
        
        Returns:
            Dictionary mapping entity names to EntityData
        """
        if self.logger:
            self.logger.info("Loading all L_raw entities")

        parquet_files = list(self.lraw_dir.glob("*.parquet"))

        if not parquet_files:
            raise ValueError(f"No Parquet files found in {self.lraw_dir}")

        # Track which canonical entities we've already loaded to avoid duplicates via aliases
        loaded_canonical_names = set()

        for parquet_file in parquet_files:
            raw_entity_name = parquet_file.stem  # Remove .parquet extension
            canonical_name = self.ENTITY_NAME_ALIASES.get(raw_entity_name, raw_entity_name)

            # If this is an alias and the canonical file also exists, prefer the canonical once
            if canonical_name in loaded_canonical_names:
                if self.logger:
                    self.logger.debug(
                        f"Skipping alias entity '{raw_entity_name}' (canonical '{canonical_name}' already loaded)"
                    )
                continue

            entity_name = raw_entity_name
            try:
                entity_data = self.load_entity(entity_name)
                # If an alias was used, normalize the entity name in memory to the canonical one
                if canonical_name != entity_name:
                    entity_data.entity_name = canonical_name
                    entity_name = canonical_name
                self.entities[entity_name] = entity_data
                loaded_canonical_names.add(entity_name)

                if self.logger:
                    self.logger.debug(
                        f"Loaded entity: {entity_name}",
                        rows=entity_data.row_count,
                        columns=entity_data.column_count
                    )
            except Exception as e:
                if self.logger:
                    self.logger.error(
                        f"Failed to load entity: {entity_name}",
                        exception=e
                    )
                raise

        if self.logger:
            self.logger.info(
                f"Loaded {len(self.entities)} entities from L_raw",
                entities=list(self.entities.keys())
            )

        return self.entities
    
    def load_entity(self, entity_name: str) -> EntityData:
        """
        Load specific entity from Parquet file.
        
        Args:
            entity_name: Name of entity (without .parquet extension)
        
        Returns:
            EntityData instance
        """
        parquet_file = self.lraw_dir / f"{entity_name}.parquet"
        
        if not parquet_file.exists():
            raise FileNotFoundError(f"Entity file not found: {parquet_file}")
        
        # Read Parquet file
        df = pd.read_parquet(parquet_file)
        
        # Normalize entity name using aliases for schema/PK resolution
        canonical_name = self.ENTITY_NAME_ALIASES.get(entity_name, entity_name)
        
        # Resolve primary key with fallbacks
        primary_key = self.ENTITY_SCHEMA.get(canonical_name, f"{canonical_name}_id")
        if primary_key not in df.columns:
            # Try known aliases
            for alt in self.PRIMARY_KEY_ALIASES.get(canonical_name, []) + [f"{entity_name}_id", f"{canonical_name}_id", 'id']:
                if alt in df.columns:
                    primary_key = alt
                    break
        
        # Special handling: synthesize PK for known composite-key entities
        if canonical_name == 'faculty_course_competency' and primary_key not in df.columns:
            # If faculty_id and course_id exist, create a deterministic composite key
            if 'faculty_id' in df.columns and 'course_id' in df.columns:
                primary_key = 'competency_id'
                df[primary_key] = df['faculty_id'].astype(str) + '__' + df['course_id'].astype(str)
            else:
                # Fallback: create a surrogate key
                primary_key = 'competency_id'
                df[primary_key] = [f"FCC_{i:08d}" for i in range(len(df))]
        
        # If still missing, create a surrogate PK column
        if primary_key not in df.columns:
            primary_key = self.ENTITY_SCHEMA.get(canonical_name, f"{canonical_name}_id")
            if self.logger:
                self.logger.warning(
                    f"Primary key '{primary_key}' not found for entity '{canonical_name}'. Creating surrogate IDs."
                )
            df[primary_key] = [f"{canonical_name}_{i:08d}" for i in range(len(df))]
        
        # Ensure PK non-null and unique; fix if required
        if df[primary_key].isnull().any():
            if self.logger:
                self.logger.warning(
                    f"Null values found in primary key '{primary_key}' for entity '{canonical_name}'. Filling with surrogates."
                )
            df[primary_key] = df[primary_key].fillna(method='ffill')
            # If still null at the start, fill remaining with unique surrogates
            df[primary_key] = df[primary_key].fillna(
                pd.Series([f"{canonical_name}_{i:08d}" for i in range(len(df))])
            )
        
        if df[primary_key].duplicated().any():
            if self.logger:
                self.logger.warning(
                    f"Duplicate values detected in primary key '{primary_key}' for entity '{canonical_name}'. Making them unique."
                )
            # Disambiguate duplicates by appending a running index per value
            counts = {}
            new_vals = []
            for val in df[primary_key].astype(str).tolist():
                cnt = counts.get(val, 0)
                if cnt == 0:
                    new_vals.append(val)
                else:
                    new_vals.append(f"{val}__{cnt}")
                counts[val] = cnt + 1
            df[primary_key] = new_vals
        
        # Detect foreign keys (columns ending with _id that are not the primary key)
        foreign_keys = [
            col for col in df.columns
            if col.endswith('_id') and col != primary_key
        ]
        
        # Create EntityData
        entity_data = EntityData(
            entity_name=canonical_name,
            data=df,
            row_count=len(df),
            column_count=len(df.columns),
            primary_key=primary_key,
            foreign_keys=foreign_keys
        )
        
        # Validate (non-fatal): if validation fails even after healing, log and proceed
        if not entity_data.validate():
            if self.logger:
                self.logger.warning(
                    f"Entity validation non-fatal failure after healing: {canonical_name}",
                    primary_key=primary_key
                )
        
        return entity_data
    
    def get_entity(self, entity_name: str) -> Optional[EntityData]:
        """
        Get loaded entity by name.
        
        Args:
            entity_name: Name of entity
        
        Returns:
            EntityData if loaded, None otherwise
        """
        return self.entities.get(entity_name)
    
    def get_entity_dataframe(self, entity_name: str) -> Optional[pd.DataFrame]:
        """
        Get entity DataFrame by name.
        
        Args:
            entity_name: Name of entity
        
        Returns:
            DataFrame if loaded, None otherwise
        """
        entity = self.get_entity(entity_name)
        return entity.data if entity else None
    
    def validate_all_entities(self) -> Dict[str, bool]:
        """
        Validate all loaded entities.
        
        Returns:
            Dictionary mapping entity names to validation results
        """
        validation_results = {}
        
        for entity_name, entity_data in self.entities.items():
            validation_results[entity_name] = entity_data.validate()
        
        if self.logger:
            failed = [name for name, result in validation_results.items() if not result]
            if failed:
                self.logger.warning(
                    f"Entity validation failed for: {failed}",
                    failed_entities=failed
                )
            else:
                self.logger.info("All entities validated successfully")
        
        return validation_results
    
    def get_summary(self) -> Dict[str, Any]:
        """
        Get summary of loaded entities.
        
        Returns:
            Summary dictionary
        """
        summary = {
            "total_entities": len(self.entities),
            "entities": {},
            "total_rows": 0,
            "total_columns": 0,
        }
        
        for entity_name, entity_data in self.entities.items():
            summary["entities"][entity_name] = {
                "rows": entity_data.row_count,
                "columns": entity_data.column_count,
                "primary_key": entity_data.primary_key,
                "foreign_keys": entity_data.foreign_keys,
            }
            summary["total_rows"] += entity_data.row_count
            summary["total_columns"] += entity_data.column_count
        
        return summary
    
    def extract_core_entities(self) -> Dict[str, pd.DataFrame]:
        """
        Extract core entities required for scheduling.
        
        Returns:
            Dictionary of core entity DataFrames
        """
        core_entities = [
            'institutions', 'departments', 'programs', 'courses',
            'shifts', 'time_slots', 'faculty', 'rooms', 'students',
            'batches', 'faculty_course_competency'
        ]
        
        extracted = {}
        for entity_name in core_entities:
            df = self.get_entity_dataframe(entity_name)
            if df is not None:
                extracted[entity_name] = df
            elif self.logger:
                self.logger.warning(f"Core entity not found: {entity_name}")
        
        return extracted


